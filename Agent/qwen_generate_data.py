import json
import time
from dashscope import Generation
import dashscope

dashscope.api_key = 'sk-e05e5076e72e493998428e2d770e7a11'

# ========== 1ï¸âƒ£ å®šä¹‰åŸºç¡€ Persona æ•°æ® (æ¨¡æ‹Ÿ Syntheticâ€‘Personaâ€‘Chat ç¤ºä¾‹) ==========
base_personas = [
    {
        "persona_a": "I am a university student studying computer science. I'm curious and polite.",
        "persona_b": "I am a professor who supervises graduate students, structured and encouraging."
    },
    {
        "persona_a": "I am a nurse working night shifts, empathetic and friendly.",
        "persona_b": "I am a doctor at the same hospital, focused and professional."
    }
]

# ========== 2ï¸âƒ£ å®šä¹‰ç¤¾ä¼šå› ç´  (social factors) ==========
social_factors = [
    {"relation": "superior-subordinate", "formality": "formal", "context": "office"},
    {"relation": "peer-peer", "formality": "informal", "context": "cafe"},
    {"relation": "stranger", "formality": "neutral", "context": "conference"}
]

# ========== 3ï¸âƒ£ è°ƒç”¨ Qwen æ¨¡å‹çš„å°è£…å‡½æ•° ==========
def qwen_generate(prompt: str, retry: int = 2) -> str:
    """æ›´ç¨³å¥çš„ Qwen è°ƒç”¨å°è£…ï¼Œå…¼å®¹å¤šç§è¿”å›ç»“æ„"""
    for attempt in range(retry):
        try:
            response = Generation.call(
                model="qwen-max",
                prompt=prompt,
                result_format="message"
            )

            # --- Debug ç”¨ï¼šæŸ¥çœ‹åŸå§‹ç»“æ„ ---
            # print(json.dumps(response, indent=2, ensure_ascii=False))

            if not response:
                raise ValueError("Empty response from DashScope")

            # Case 1: æ ‡å‡† text å­—æ®µ
            if isinstance(response, dict):
                output = response.get("output", {})
                if isinstance(output, dict):
                    # ä¼˜å…ˆä» text å–ç»“æœ
                    text = output.get("text")
                    if isinstance(text, str) and text.strip():
                        return text.strip()

                    # Case 2: ä» message choice é‡Œæå–
                    choices = output.get("choices")
                    if choices and isinstance(choices, list):
                        msg = choices[0].get("message", {})
                        content = msg.get("content")
                        if content and isinstance(content, str):
                            return content.strip()

            # Case 3: fallback - æ‰“å°æ•´ä¸ªå¯¹è±¡
            print("âš ï¸ æœªæ‰¾åˆ°æ–‡æœ¬å­—æ®µï¼Œè¿”å›åŸå§‹æ•°æ®ç»“æ„")
            return json.dumps(response, ensure_ascii=False)

        except Exception as e:
            print(f"âš ï¸ Qwen è°ƒç”¨å¤±è´¥ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰: {e}")
            time.sleep(1)

    return "[ERROR: EMPTY_RESPONSE]"

# ========== 4ï¸âƒ£ Prompt æ¨¡æ¿ï¼šä¸¤ä¸ªè§’è‰² ==========
def compose_prompt(persona, partner_persona, factors, role="user"):
    role_desc = "You are the AR glasses user" if role == "user" else "You are the conversation partner"
    return f"""
{role_desc} engaged in a {factors['formality']} conversation with a {factors['relation']} at a {factors['context']}.
Your persona: {persona}
Your partner's persona: {partner_persona}
Speak naturally, stay in character, under 50 words.
"""

# ========== 5ï¸âƒ£ åŒ LLM å¯¹è¯æ¨¡æ‹Ÿ ==========
def simulate_conversation(p_a, p_b, factors, rounds=4):
    conv = []
    user_prompt = compose_prompt(p_a, p_b, factors, "user")
    partner_prompt = compose_prompt(p_b, p_a, factors, "partner")
    last_reply = ""

    for i in range(rounds):
        # User Agent
        user_input = user_prompt + (f"\nPartner said: {last_reply}" if last_reply else "")
        user_resp = qwen_generate(user_input)
        conv.append({"speaker": "User", "text": user_resp})

        # Partner Agent
        partner_input = partner_prompt + f"\nUser said: {user_resp}"
        partner_resp = qwen_generate(partner_input)
        conv.append({"speaker": "Partner", "text": partner_resp})

        last_reply = partner_resp or last_reply
        print(f"ğŸ—¨ï¸ Round {i+1} done.")
    return conv

# ========== 6ï¸âƒ£ ç¬¬ä¸‰æ¨¡å‹ç”Ÿæˆç¤¾äº¤å»ºè®® ==========
def generate_social_advice(conversation, factors):
    joined_dialogue = "\n".join([f"{c['speaker']}: {c['text']}" for c in conversation])
    prompt = f"""
You are a social behavior assistant.
Given the following conversation and social context,
provide one concise behavioral tip for the AR glasses user
(e.g., tone, body language, response timing).

Context: {factors}
Conversation:
{joined_dialogue}
"""
    return qwen_generate(prompt)

# ========== 7ï¸âƒ£ ä¸»æµç¨‹ï¼šæ„å»ºç¤¾äº¤ç¼“å­˜ ==========
def build_social_cache():
    social_cache = []
    for idx, personas in enumerate(base_personas):
        for jdx, factors in enumerate(social_factors):
            print(f"\n=== æ¨¡æ‹Ÿåœºæ™¯ [{idx+1}-{jdx+1}] ===")
            dialogue = simulate_conversation(personas["persona_a"], personas["persona_b"], factors)
            advice = generate_social_advice(dialogue, factors)
            entry = {
                "context_key": dialogue[-1]["text"] if dialogue else "",
                "social_advice": advice,
                "social_factors": factors,
                "dialogue": dialogue
            }
            social_cache.append(entry)
    return social_cache

# ========== 8ï¸âƒ£ æ‰§è¡Œå¹¶ä¿å­˜ç»“æœ ==========
if __name__ == "__main__":
    data = build_social_cache()
    with open("socialmind_qwen_cache.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å·²ç”Ÿæˆ {len(data)} æ¡ç¤¾äº¤å¯¹è¯æ ·æœ¬ï¼Œç»“æœä¿å­˜åœ¨ socialmind_qwen_cache.json ä¸­ã€‚")